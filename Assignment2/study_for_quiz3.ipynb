{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 핵심파트\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, seq_len_max):\n",
    "\n",
    "        PE = torch.zeros(seq_len_max, dim)\n",
    "\n",
    "        position = torch.arange(seq_len_max, dim)\n",
    "        position = torch.arange(seq_len_max, dim)\n",
    "        # 중요!!!!!\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "\n",
    "        scores = torch.M\n",
    "\n",
    "        PE[:, 0::2] = torch.sin(position * div_term)\n",
    "        PE[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('PE', PE.unsqueeze(0))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.PE[:,  :X.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드 어텐션 핵심파트 \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, head_num):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.head_num = head_num\n",
    "        self.word_dim = dim // head_num\n",
    "\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "\n",
    "    def scaled_dot_product(self, Q, K, V, mask = None):\n",
    "        # 중요\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.word_dim).float())\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) \n",
    "\n",
    "        probs = nn.functional.softmax(scores, dim = -1)\n",
    "\n",
    "        heads = torch.matmul(probs, V) # V와 matmul 수행\n",
    "\n",
    "        return heads\n",
    "    \n",
    "\n",
    "    def split(self, X): # 나누기\n",
    "        print()\n",
    "    \n",
    "    def combine(self, X): # 합치기\n",
    "        print()\n",
    "\n",
    "    def forward(self, X_Q, X_K, X_V, mask = None):\n",
    "        Q = self.split(self.W_q(X_Q))\n",
    "        K = self.split(self.W_k(X_K))\n",
    "        V = self.split(self.W_v(X_V))\n",
    "\n",
    "        heads = self.scaled_dot_product(Q, K, V, mask) # 중요(scaled dot product 수행)\n",
    "        output = self.W_o(self.combine(heads)) # 중요(Q, K, V 합친 heads를 output에 mapping)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 인코더 핵심파트\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, FFN_dim):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.FFN_layer = nn.Sequential(nn.Linear(dim, FFN_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(FFN_dim, dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.FFN_layer(X)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(dim, head_num)\n",
    "\n",
    "        self.ffn = FFN(dim, FFN_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        # 인코더 1. 멀티헤드 셀프 어텐션 통과\n",
    "        attn_output = self.attention(X, X, X, mask) # Q, K, V all use the same X.\n",
    "        \n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        X = self.norm1(X + attn_output) # Residual Connection 적용\n",
    "\n",
    "        # 인코더 2. FFN 통과\n",
    "        ffn_output = nn.FFN(X)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        output = self.norm2(X + ffn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 디코더 핵심파트\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 1. 먼저 셀프 어텐션부터\n",
    "        self.self_attention = MultiHeadAttention(dim, head_num)\n",
    "\n",
    "\n",
    "        # 2. 마스크드 멀티헤드 크로스 어텐션\n",
    "        self.cross_attention = MultiHeadAttention(dim, head_num)\n",
    "\n",
    "\n",
    "        # 3. FFN\n",
    "        self.ffn = FFN(dim, FFN_dim)\n",
    "\n",
    "\n",
    "        # 4. LayerNorm Layers\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, enc_output, cross_attn_mask, self_attn_mask):\n",
    "        # 1. 셀프어텐션 with 잔차 연결\n",
    "        self_attn_output = self.self_attention(X, X, X, self_attn_mask)\n",
    "        self_attn_output = self.dropout(self_attn_output)\n",
    "        X = self.norm1(X + self_attn_output)\n",
    "\n",
    "        # 2. 크로스어텐션 with 잔차 연결\n",
    "        cross_attn_output = self.cross_attention(X, enc_output, enc_output, cross_attn_mask) # 가장중요\n",
    "        cross_attn_output = self.dropout(cross_attn_output)\n",
    "        X = self.norm2(X + cross_attn_output)\n",
    "\n",
    "        # 3. FFN with 잔차 연결\n",
    "        ffn_output = self.ffn(X)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        output = self.norm3(X + ffn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패치 임베딩\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size = 224, patch_size = 16, in_chans = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size) # 중요\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "\n",
    "        # 중요\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size) # 커널 사이즈, 스트라이드 모두 패치 사이즈만함.\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape # 배치, 채널, 높이, 너비 순\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3) # for q, k, v\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale # Query, Key 간 연산.\n",
    "        attn = attn.softmax(dim = -1) # softmax 통과\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None, act_layer = nn.GELU): # activation으로 nn.GELU 사용함에 유의\n",
    "        super.__init__()\n",
    "\n",
    "        out_features = out_features or hidden_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio = 4., act_layer = nn.GELU, norm_layer = nn.LayerNorm):\n",
    "        super.__init__()\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features = dim, hidden_features = mlp_hidden_dim, act_layer = act_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1. Residual Connection for Self-Attention\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        # 2. Residual Connection for MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=10, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "\n",
    "        # 1. 패치임베딩\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # 2. CLS 토큰 추가\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # 3. 포지셔널 임베딩 : 중요. -> define a learnable positional embedding that matches the patchified input token size.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        # 4. 트랜스포머 블록\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,  norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        # 5. 최종 정규화\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # 6. 분류기 헤드\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0] # 배치 사이즈 따오기, CLS 토큰에 필요함\n",
    "\n",
    "        # 1. 패치 임베딩\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # 2. CLS 토큰 추가 - learnable param\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "\n",
    "        # 3. 포지셔널 임베딩 추가\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 4. 트랜스포머 블록 통과(각 블록 모듈 순차통과)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # 5. 최종 정규화 - 출력 텐서 정규화 적용\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 6. CLS 토큰 최종 출력\n",
    "        cls_token_final = x[:, 0]\n",
    "\n",
    "        # 7. 분류기 헤드에 전달\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
