{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2150188401(2) Artificial Intelligence Assignment #2<br>  Transformer from scratch (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Computer Science & Engineering, Soongsil University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Haneul Pyeon, September 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully \n",
    "look at given PDF file.**\n",
    "\n",
    "In this notebook, you will learn to implement a transformer model from scratch. By doing so, you will understand the nuts and bolts of Transformers more clearly at a code level.\n",
    "<br>\n",
    "There are **5 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "\n",
    "### Some helpful tutorials and references for assignment #2-1:\n",
    "- [1] Original Transformer paper(Vaswani et al., 2017). [[link]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
    "- [2] Helpful instructions about how Transformer works. [[link]](https://github.com/jadore801120/attention-is-all-you-need-pytorch)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check virtual env and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "assert os.environ[\"CONDA_DEFAULT_ENV\"] == \"AI-24\", \"current environment is not AI-24\"\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() is True:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder](./imgs/Model_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original paper on Transformer, positional encoding is constructed by using sine functions to even dimensions and cosine functions to odd dimensions.\n",
    "\n",
    "\\begin{align*}\n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/dim}) \\\\\n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/dim})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.arange [[link]](https://pytorch.org/docs/stable/generated/torch.arange.html)   \n",
    "Returns a 1-D tensor of size \n",
    "$$\n",
    "\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\n",
    "$$ with values from the interval [start, end) taken with common difference 'step' beginning from 'start'.\n",
    "\n",
    "Note that non-integer 'step' is subject to floating point rounding errors when comparing against 'end'; to avoid inconsistency, we advise subtracting a small epsilon from 'end' in such cases.\n",
    "\n",
    "$$\n",
    "\\text{out}_{i+1} = \\text{out}_i + \\text{step}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter \n",
    "- start (Number) – the starting value for the set of points. Default: 0.\n",
    "\n",
    "- end (Number) – the ending value for the set of points\n",
    "\n",
    "- step (Number) – the gap between each pair of adjacent points. Default: 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1.0000, 1.5000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "example_arange1 = torch.arange(5)\n",
    "print(example_arange1)\n",
    "example_arange2 = torch.arange(1, 4)\n",
    "print(example_arange2)\n",
    "example_arange3 = torch.arange(1, 2.5, 0.5)\n",
    "print(example_arange3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.pow [[link]](https://pytorch.org/docs/stable/generated/torch.pow.html)   \n",
    "\n",
    "Takes the power of each element in input with exponent and returns a tensor with the result.\n",
    "\n",
    "exponent can be either a single float number or a Tensor with the same number of elements as input.\n",
    "\n",
    "When exponent is a scalar value, the operation applied is:\n",
    "\n",
    "$$\n",
    "\\text{out}_i = x_i^{\\text{exponent}}\n",
    "$$\n",
    " \n",
    "When exponent is a tensor, the operation applied is:\n",
    "\n",
    "$$\n",
    "\\text{out}_i = x_i^{\\text{exponent}_i}\n",
    "$$\n",
    "​\n",
    " \n",
    "When exponent is a tensor, the shapes of input and exponent must be broadcastable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- input (Tensor) – the input tensor.\n",
    "\n",
    "- exponent (float or tensor) – the exponent value\n",
    "\n",
    "#### Keyword Arguments\n",
    "- out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2147, -0.9597, -0.2090, -0.4159])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4755, 0.9211, 0.0437, 0.1730])\n"
     ]
    }
   ],
   "source": [
    "exm_pow = torch.pow(a, 2)\n",
    "print(exm_pow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/hqhgf4kx2811y6vbkgxmp7f00000gn/T/ipykernel_56935/2055934660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "b = torch.arange(1., 5.)\n",
    "print(b)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/hqhgf4kx2811y6vbkgxmp7f00000gn/T/ipykernel_56935/877542064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexm_pow2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexm_pow2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "exm_pow2 = torch.pow(b, exp)\n",
    "print(exm_pow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   4.,  27., 256.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "a = torch.randn(4)\n",
    "a\n",
    "torch.pow(a, 2)\n",
    "\n",
    "exp = torch.arange(1., 5.)\n",
    "a = torch.arange(1., 5.)\n",
    "a\n",
    "exp\n",
    "torch.pow(a, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.sin [[link]](https://pytorch.org/docs/stable/generated/torch.sin.html)   \n",
    "Returns a new tensor with the sine of the elements of `input`.\n",
    "\n",
    "$$\n",
    "\\text{out}_i = \\sin(\\text{input}_i)\n",
    "$$\n",
    "\n",
    "#### Parameters\n",
    "- `input` (Tensor): the input tensor.\n",
    "\n",
    "#### Keyword Arguments\n",
    "- `out` (Tensor, optional): the output tensor.\n",
    "\n",
    "\n",
    "### torch.cos [[link]](https://pytorch.org/docs/stable/generated/torch.cos.html)   \n",
    "\n",
    "Returns a new tensor with the cosine of the elements of input.\n",
    "\n",
    "$$ \n",
    "\\text{out}_i = \\sin(\\text{input}_i)\n",
    "$$\n",
    "\n",
    "#### Parameters\n",
    "input (Tensor) – the input tensor.\n",
    "\n",
    "#### Keyword Arguments\n",
    "out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, seq_len_max):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        PE = torch.zeros(seq_len_max, dim) # zeros : Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        position = torch.arange(0, seq_len_max, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        \n",
    "        PE[:, 0::2] = torch.sin(position * div_term)\n",
    "        PE[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        ######################### DO NOT CHANGE #########################\n",
    "        # Positional Encoding is not learnable parameters.\n",
    "        self.register_buffer('PE', PE.unsqueeze(0))\n",
    "        ######################### DO NOT CHANGE #########################\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X + self.PE[:, :X.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multi_head_attention](./imgs/Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement MultiHeadAttention Class.  \n",
    "The parameters of MultiHeadAttention class is defined as follows. \n",
    "Note that according to the definition of multi-head attention, the dimension of the model is equal to the product\n",
    "of the word dimension and the number of heads\n",
    "\n",
    "$dim$:  dimension of the model  \n",
    "$dim$ = dimension for a each word * $head\\_num$  \n",
    "$seq\\_len$:  length of the input sequence\n",
    "\n",
    "This module will get batched sequences x and return multi-head attention ouput. \n",
    "\n",
    "X size:  $(batch\\_num, seq\\_len, dim)$  \n",
    "mask: Tensor to indicate the words involved in score calculation  \n",
    "output size:  $(batch\\_num, seq\\_len, dim)$\n",
    "\n",
    "$W_q$ = linear transformation for query  \n",
    "$W_k$ = linear transformation for key    \n",
    "$W_v$ = linear transformation for value  \n",
    "$W_o$ = linear transformation for concatenated heads\n",
    "\n",
    "The model operates according to the following equation.  \n",
    "It should select the values that will participate in score calculation based on the received mask.\n",
    "\n",
    "$Q = X * W_q$  \n",
    "$K = X * W_k$  \n",
    "$V = X * W_v$  \n",
    "\n",
    "$scores = \\frac{QK^T}{\\sqrt{word\\_dim}}$  \n",
    "$masked\\_scores = mask(\\frac{QK^T}{\\sqrt{word\\_dim}})$  \n",
    "$probs = softmax(masked\\_scores)$  \n",
    "$heads = probsV$  \n",
    "$output = heads * W_o$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.matmul [[link]](https://pytorch.org/docs/stable/generated/torch.matmul.html) \n",
    "\n",
    "Matrix product of two tensors.\n",
    "\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "\n",
    "- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "\n",
    "- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "\n",
    "- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "\n",
    "- If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "\n",
    "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where \\(N > 2\\)), then a batched matrix multiply is returned. \n",
    "If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. \n",
    "If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiply and removed after. \n",
    "The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). \n",
    "For example, if `input` is a $$ (j \\times 1 \\times n \\times n) $$ \n",
    "tensor and `other` is a $$ (k \\times n \\times n) $$ tensor, `out` will be a $$ (j \\times k \\times n \\times n) $$ tensor.\n",
    "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions. \n",
    "For example, if `input` is a $$ (j \\times 1 \\times n \\times m) $$ tensor and `other` is a $$ (k \\times m \\times p) $$ \n",
    "tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the matrix dimensions) are different. `out` will be a  $$ (j \\times k \\times n \\times p) $$ tensor.\n",
    "\n",
    "This operation has support for arguments with sparse layouts. In particular the matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions as torch.mm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- input (Tensor) – the first tensor to be multiplied\n",
    "\n",
    "- other (Tensor) – the second tensor to be multiplied\n",
    "\n",
    "#### Keyword Arguments\n",
    "- out (Tensor, optional) – the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, head_num):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.head_num = head_num\n",
    "        self.word_dim = dim // head_num\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "         # Weight matrices for the linear transformation of Query, Key, and Value (dim x dim)\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        # Weight matrix for the linear transformation after combining the multi-head attention results\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        ######################### TO DO #########################\n",
    "         # Calculate the dot-product between Q and K, then scale by the square root of word_dim\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.word_dim).float())\n",
    "        \n",
    "        # If a mask is provided, apply the mask (masked positions will not attend)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # Apply the softmax function to calculate the attention probabilities    \n",
    "        probs = nn.functional.softmax(scores, dim = -1)\n",
    "        \n",
    "         # Multiply the attention probabilities by V to get the final attention values (heads)\n",
    "        heads = torch.matmul(probs, V)\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return heads\n",
    "    \n",
    "    def split(self, X):\n",
    "        batch_num, seq_len, dim = X.size()\n",
    "        return X.view(batch_num, seq_len, self.head_num, self.word_dim).transpose(1, 2)\n",
    "        \n",
    "    def combine(self, X):\n",
    "        batch_num, _, seq_len, _ = X.size()\n",
    "        return X.transpose(1, 2).contiguous().view(batch_num, seq_len, self.dim)\n",
    "        \n",
    "    def forward(self, X_Q, X_K, X_V, mask=None):\n",
    "        Q = self.split(self.W_q(X_Q))\n",
    "        K = self.split(self.W_k(X_K))\n",
    "        V = self.split(self.W_v(X_V))\n",
    "        \n",
    "        heads = self.scaled_dot_product(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine(heads))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement EncoderLayer class using **one MultiHeadAttention layer, one FNN layer and two normalization layer**.  \n",
    "**Please apply dropout right after passing through multi-head attention and FFN layer.**\n",
    "\n",
    "**HINT**  \n",
    "**1. Normalization is a LayerNorm.**  \n",
    "**2. LayerNorm layers have learnable parameters. Therefore, you should use two normalization layers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm  [[link]](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) \n",
    "\n",
    "Applies Layer Normalization over a mini-batch of inputs.\n",
    "\n",
    "This layer implements the operation as described in the paper *Layer Normalization*:\n",
    "\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\times \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "The mean (\\(\\mathbb{E}[x]\\)) and standard deviation (\\(\\text{Var}[x]\\)) are calculated over the last \\(D\\) dimensions, where \\(D\\) is the dimension of `normalized_shape`. \n",
    "\n",
    "For example, if `normalized_shape` is \\((3, 5)\\) (a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e. `input.mean((-2, -1))`).\n",
    "\n",
    " \\(\\gamma\\) and \\(\\beta\\) are learnable affine transform parameters of `normalized_shape` if `elementwise_affine` is `True`. The standard deviation is calculated via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "- **normalized_shape** (int or list or torch.Size): input shape from an expected input of size\n",
    "$$\n",
    "[* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1] \\times \\dots \\times \\text{normalized\\_shape}[-1]]\n",
    "$$\n",
    "\n",
    "If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension, which is expected to be of that specific size.\n",
    "- **eps** (float): a value added to the denominator for numerical stability. Default: 1e-5\n",
    "\n",
    "- **elementwise_affine** (bool): a boolean value that, when set to `True`, allows this module to have learnable per-element affine parameters, initialized to ones (for weights) and zeros (for biases). Default: `True`.\n",
    "\n",
    "- **bias** (bool): If set to `False`, the layer will not learn an additive bias (only relevant if `elementwise_affine` is `True`). Default: `True`.\n",
    "\n",
    "#### Variables\n",
    "\n",
    "- **weight**: the learnable weights of the module of shape \n",
    "  \\[\n",
    "  \\text{normalized\\_shape}\n",
    "  \\]\n",
    "  when `elementwise_affine` is set to `True`. The values are initialized to 1.\n",
    "\n",
    "- **bias**: the learnable bias of the module of shape \n",
    "  \\[\n",
    "  \\text{normalized\\_shape}\n",
    "  \\]\n",
    "  when `elementwise_affine` is set to `True`. The values are initialized to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/hqhgf4kx2811y6vbkgxmp7f00000gn/T/ipykernel_720/910907886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NLP Example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlayer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "nlp_output = layer_norm(embedding)\n",
    "print(\"NLP Output:\")\n",
    "print(nlp_output)\n",
    "\n",
    "# Image Example\n",
    "N, C, H, W = 20, 5, 10, 10\n",
    "input_tensor = torch.randn(N, C, H, W)\n",
    "layer_norm_image = nn.LayerNorm([C, H, W])\n",
    "output_image = layer_norm_image(input_tensor)\n",
    "print(\"\\nImage Output:\")\n",
    "print(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout [[link]](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) \n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability \\(p\\).\n",
    "\n",
    "The zeroed elements are chosen independently for each forward call and are sampled from a Bernoulli distribution.\n",
    "\n",
    "Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper *Improving neural networks by preventing co-adaptation of feature detectors*.\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of \n",
    "\n",
    "$$\n",
    "\\frac{1}{1 - p}\n",
    "$$\n",
    "\n",
    "during training. This means that during evaluation the module simply computes an identity function.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **p** (float): probability of an element to be zeroed. Default: 0.5\n",
    "- **inplace** (bool): If set to `True`, will do this operation in-place. Default: `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, -2.5396,  0.5204,  0.2327,  0.0000,  1.0036, -0.0000, -0.0000,\n",
      "         -2.3482, -0.0377, -0.3796, -1.2440, -0.5376, -0.0000, -2.3645, -2.2247],\n",
      "        [-1.6280,  0.0000,  0.5182, -1.6249, -0.0884,  2.4014, -1.5832,  1.8526,\n",
      "         -1.4817, -1.6126, -0.2854, -0.0000,  2.3904,  0.0000, -0.0544,  0.4426],\n",
      "        [-4.2613,  0.7616,  0.9399, -0.0156,  1.9415, -1.1989, -0.0697,  0.0000,\n",
      "         -0.7403, -1.5608,  0.3956, -1.6576, -0.6955,  0.9949,  2.5151, -0.5352],\n",
      "        [ 2.3932, -0.0000,  0.4580, -0.0000, -0.0000,  1.9528, -0.0000, -0.8795,\n",
      "         -0.8880, -0.2050,  1.8605, -1.2629,  0.6634,  1.7848,  0.0000,  1.8582],\n",
      "        [ 0.0000, -0.0000,  1.1719, -1.4917,  0.3689,  1.3155,  0.4968, -1.1018,\n",
      "         -1.4754, -0.4517,  0.0857,  0.8364,  0.0777, -0.0000, -2.8265, -0.0000],\n",
      "        [-1.1403, -0.5473, -0.3105,  2.5396,  1.3312, -1.6657,  1.4482,  0.0000,\n",
      "         -0.5993,  0.4486,  1.5249,  0.3322,  0.7249,  0.0000,  0.9512,  0.6757],\n",
      "        [-0.8295, -0.0000, -0.0252,  0.9194,  0.0586, -0.9979,  0.4818,  1.0692,\n",
      "          0.1040,  0.0603, -1.9991, -2.3644, -0.9578,  1.0311,  0.0000, -0.3537],\n",
      "        [ 0.1163,  0.0000,  0.4720,  0.3435, -0.8803, -2.0511,  0.0263,  1.0344,\n",
      "          0.0000,  0.0000, -2.4879, -0.8227,  0.2440, -1.7550,  0.8024, -0.1315],\n",
      "        [ 1.3752, -1.7729, -0.0000,  1.5764,  0.0045, -0.1831,  0.8880,  0.2217,\n",
      "          1.1455, -0.0000, -1.1442,  0.4880, -2.2811, -0.0000, -0.3080, -0.4899],\n",
      "        [-1.0988, -0.8359, -1.1813,  0.5815,  1.5259, -0.8398,  0.9560,  0.6463,\n",
      "          0.6388, -2.4930,  0.8245,  0.3975, -0.0000, -1.3660, -0.0000, -0.0000],\n",
      "        [ 0.9036, -0.8805, -0.0000,  0.0000,  1.3069,  0.4301,  0.0000, -0.5122,\n",
      "          0.7788,  1.3093,  0.0000, -0.0813, -0.5412, -2.5175,  0.0325,  0.5492],\n",
      "        [-0.2496,  1.2209, -0.1573,  0.2925, -2.0444,  0.7027,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0234, -1.8521,  0.0000, -0.0000,  0.9052,  0.0000],\n",
      "        [-0.0000,  0.8287, -0.0000, -0.0000,  0.0000,  0.2137, -0.5523, -1.0801,\n",
      "         -0.2561, -1.0685,  0.2967,  0.0000, -1.0422, -0.6210, -0.1848, -0.2511],\n",
      "        [ 0.1544,  1.0395, -1.9953, -0.2386, -0.8377, -1.3888, -1.8138,  0.0000,\n",
      "         -0.5083, -0.4023,  1.4088, -0.0000,  2.3856, -1.6826, -1.0925,  0.2575],\n",
      "        [ 0.1043, -0.0000,  0.5294,  0.0000,  0.3641, -0.7451, -0.4063, -0.2515,\n",
      "          0.0114, -0.6062, -0.6478,  1.0172,  1.1984, -0.1389,  0.0836,  3.0307],\n",
      "        [ 0.5048, -0.9907, -1.0739, -0.6528, -0.7883,  1.0028, -0.0000, -1.5486,\n",
      "         -0.0000, -1.6235, -3.4987,  0.5493, -0.0000, -3.6148, -0.0000, -1.0753],\n",
      "        [ 0.4822, -0.2011, -0.8013,  1.1036, -1.1528, -2.7332, -0.3359, -2.6323,\n",
      "          2.2039, -0.1079,  1.8487,  0.0000,  0.0684, -0.0000,  0.4336,  0.0000],\n",
      "        [-0.5732,  1.3079, -0.3824, -1.3290, -0.5503,  0.4202, -0.3066, -0.0000,\n",
      "         -3.0967,  0.9066,  0.3813,  0.4883, -0.0261,  0.0692, -0.0000,  1.0447],\n",
      "        [-0.5244, -2.0823, -2.0879,  0.4391,  0.6138,  0.0000, -1.5155, -1.3601,\n",
      "          1.9648, -0.4933,  0.0000,  0.0272, -0.9288, -1.2027,  1.1949, -0.1183],\n",
      "        [ 0.1689, -0.6736, -0.1656,  0.0520, -1.4255, -0.0000,  0.2235,  0.0000,\n",
      "          0.0000, -0.5052,  1.4965,  0.3448, -0.2502,  2.0378, -1.2501,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(20, 16)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, FFN_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.FFN_layer = nn.Sequential(nn.Linear(dim, FFN_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(FFN_dim, dim))\n",
    "    def forward(self, X):\n",
    "        return self.FFN_layer(X)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        # 1. Multi-Head Attention Layer\n",
    "        self.attention = MultiHeadAttention(dim, head_num)\n",
    "        \n",
    "        # 2. FFN\n",
    "        self.ffn = FFN(dim, FFN_dim)\n",
    "        \n",
    "        # 3. Layer Normalization Layers\n",
    "        self.norm1 = nn.LayerNorm(dim)  # For Attention output normalization\n",
    "        self.norm2 = nn.LayerNorm(dim)  # For FFN output normalization\n",
    "        \n",
    "        # 4. Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        attn_output = self.attention(X, X, X, mask)  # Self-attention: Q, K, V all use the same X!\n",
    "        attn_output = self.dropout(attn_output) # Applying dropout technique\n",
    "        X = self.norm1(X + attn_output)  # Residual connection and normalization\n",
    "        \n",
    "        ffn_output = self.ffn(X)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        output = self.norm2(X + ffn_output)  # Residual connection and normalization\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement DecoderLayer class using **two MultiHeadAttention layers(self-attention and cross-attention), one FNN layer and three normalization layers.** \n",
    "**Please apply dropout right after passing through two multi-head attention layers and FFN layer.**\n",
    "\n",
    "**HINT**  \n",
    "**1. Normalization is a LayerNorm.**  \n",
    "**2. LayerNorm layers have learnable parameters. Therefore, you should use three normalization layers.**  \n",
    "**3. The first multi-head attention layer is a self attention layer, and the second attention layer is a cross attention layer. Choose the mask carefully.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        # 1. First Multi-Head Attention layer (Self-Attention)\n",
    "        self.self_attention = MultiHeadAttention(dim, head_num)\n",
    "\n",
    "        # 2. Second Multi-Head Attention layer (Cross-Attention)\n",
    "        self.cross_attention = MultiHeadAttention(dim, head_num)\n",
    "\n",
    "        # 3. Feed-Forward Neural Network (FFN)\n",
    "        self.ffn = FFN(dim, FFN_dim)\n",
    "\n",
    "        # 4. Layer Normalization Layers\n",
    "        self.norm1 = nn.LayerNorm(dim)  # After Self-Attention\n",
    "        self.norm2 = nn.LayerNorm(dim)  # After Cross-Attention\n",
    "        self.norm3 = nn.LayerNorm(dim)  # After FFN\n",
    "\n",
    "        # 5. Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def forward(self, X, enc_output, cross_attn_mask, self_attn_mask):\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        # 1. Self-Attention with Residual Connection\n",
    "        self_attn_output = self.self_attention(X, X, X, self_attn_mask)\n",
    "        self_attn_output = self.dropout(self_attn_output)\n",
    "        X = self.norm1(X + self_attn_output)\n",
    "\n",
    "        # 2. Cross-Attention with Residual Connection\n",
    "        cross_attn_output = self.cross_attention(X, enc_output, enc_output, cross_attn_mask)\n",
    "        cross_attn_output = self.dropout(cross_attn_output)\n",
    "        X = self.norm2(X + cross_attn_output)\n",
    "\n",
    "        # 3. FFN with Residual Connection\n",
    "        ffn_output = self.ffn(X)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        output = self.norm3(X + ffn_output)\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare sample data and Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n",
    "                 FFN_dim, seq_len_max, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.enc_embeds = nn.Embedding(input_lib_size, dim)\n",
    "        self.dec_embeds = nn.Embedding(output_lib_size, dim)\n",
    "        self.pe = PositionalEncoding(dim, seq_len_max)\n",
    "\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(dim, head_num, FFN_dim, dropout) \\\n",
    "                                             for _ in range(layer_num)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(dim, head_num, FFN_dim, dropout) \\\n",
    "                                             for _ in range(layer_num)])\n",
    "        self.Linear = nn.Linear(dim, output_lib_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        self_attn_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        cross_attn_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal = 1)).bool()\n",
    "        nopeak_mask = nopeak_mask.to(device)\n",
    "        cross_attn_mask = cross_attn_mask & nopeak_mask\n",
    "        return self_attn_mask, cross_attn_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self_attn_mask, cross_attn_mask = self.generate_mask(src, tgt)\n",
    "        src_embeds = self.dropout(self.pe(self.enc_embeds(src)))\n",
    "        tgt_embeds = self.dropout(self.pe(self.dec_embeds(tgt)))\n",
    "\n",
    "        enc_output = src_embeds\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_output = enc_layer(enc_output, self_attn_mask)\n",
    "\n",
    "        dec_output = tgt_embeds\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_output = dec_layer(dec_output, enc_output, self_attn_mask, cross_attn_mask)\n",
    "\n",
    "        output = self.Linear(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lib_size = 5000\n",
    "output_lib_size = 5000\n",
    "dim = 512\n",
    "head_num = 4\n",
    "layer_num = 3\n",
    "FFN_dim = 2048\n",
    "seq_len_max = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n",
    "                          FFN_dim, seq_len_max, dropout)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, input_lib_size, (64, seq_len_max)).to(device)  \n",
    "tgt_data = torch.randint(1, output_lib_size, (64, seq_len_max)).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.690886497497559\n",
      "Epoch: 2, Loss: 8.590019226074219\n",
      "Epoch: 3, Loss: 8.506627082824707\n",
      "Epoch: 4, Loss: 8.452136993408203\n",
      "Epoch: 5, Loss: 8.399894714355469\n",
      "Epoch: 6, Loss: 8.356481552124023\n",
      "Epoch: 7, Loss: 8.315790176391602\n",
      "Epoch: 8, Loss: 8.261058807373047\n",
      "Epoch: 9, Loss: 8.2052640914917\n",
      "Epoch: 10, Loss: 8.15267562866211\n",
      "Epoch: 11, Loss: 8.0910005569458\n",
      "Epoch: 12, Loss: 8.024371147155762\n",
      "Epoch: 13, Loss: 7.971712589263916\n",
      "Epoch: 14, Loss: 7.912046432495117\n",
      "Epoch: 15, Loss: 7.854260444641113\n",
      "Epoch: 16, Loss: 7.795289993286133\n",
      "Epoch: 17, Loss: 7.739876747131348\n",
      "Epoch: 18, Loss: 7.677082538604736\n",
      "Epoch: 19, Loss: 7.616665363311768\n",
      "Epoch: 20, Loss: 7.553632736206055\n",
      "Epoch: 21, Loss: 7.494165420532227\n",
      "Epoch: 22, Loss: 7.4325995445251465\n",
      "Epoch: 23, Loss: 7.374553680419922\n",
      "Epoch: 24, Loss: 7.309010982513428\n",
      "Epoch: 25, Loss: 7.251531600952148\n",
      "Epoch: 26, Loss: 7.1841583251953125\n",
      "Epoch: 27, Loss: 7.119950771331787\n",
      "Epoch: 28, Loss: 7.057674407958984\n",
      "Epoch: 29, Loss: 6.993188858032227\n",
      "Epoch: 30, Loss: 6.932719707489014\n",
      "Epoch: 31, Loss: 6.865487575531006\n",
      "Epoch: 32, Loss: 6.800055027008057\n",
      "Epoch: 33, Loss: 6.740936756134033\n",
      "Epoch: 34, Loss: 6.6754865646362305\n",
      "Epoch: 35, Loss: 6.613073348999023\n",
      "Epoch: 36, Loss: 6.549443244934082\n",
      "Epoch: 37, Loss: 6.486137390136719\n",
      "Epoch: 38, Loss: 6.422045707702637\n",
      "Epoch: 39, Loss: 6.361249923706055\n",
      "Epoch: 40, Loss: 6.2957940101623535\n",
      "Epoch: 41, Loss: 6.24119234085083\n",
      "Epoch: 42, Loss: 6.173068523406982\n",
      "Epoch: 43, Loss: 6.106497287750244\n",
      "Epoch: 44, Loss: 6.052731513977051\n",
      "Epoch: 45, Loss: 5.998873710632324\n",
      "Epoch: 46, Loss: 5.937180042266846\n",
      "Epoch: 47, Loss: 5.8794426918029785\n",
      "Epoch: 48, Loss: 5.818840980529785\n",
      "Epoch: 49, Loss: 5.76046895980835\n",
      "Epoch: 50, Loss: 5.709341526031494\n",
      "Epoch: 51, Loss: 5.649889945983887\n",
      "Epoch: 52, Loss: 5.596705436706543\n",
      "Epoch: 53, Loss: 5.543420791625977\n",
      "Epoch: 54, Loss: 5.485234260559082\n",
      "Epoch: 55, Loss: 5.430046081542969\n",
      "Epoch: 56, Loss: 5.37631893157959\n",
      "Epoch: 57, Loss: 5.327680587768555\n",
      "Epoch: 58, Loss: 5.274262428283691\n",
      "Epoch: 59, Loss: 5.21945333480835\n",
      "Epoch: 60, Loss: 5.175502777099609\n",
      "Epoch: 61, Loss: 5.12172269821167\n",
      "Epoch: 62, Loss: 5.067530632019043\n",
      "Epoch: 63, Loss: 5.018698215484619\n",
      "Epoch: 64, Loss: 4.9694013595581055\n",
      "Epoch: 65, Loss: 4.919354438781738\n",
      "Epoch: 66, Loss: 4.87156867980957\n",
      "Epoch: 67, Loss: 4.824206352233887\n",
      "Epoch: 68, Loss: 4.77266788482666\n",
      "Epoch: 69, Loss: 4.726179122924805\n",
      "Epoch: 70, Loss: 4.678421974182129\n",
      "Epoch: 71, Loss: 4.631655693054199\n",
      "Epoch: 72, Loss: 4.585667610168457\n",
      "Epoch: 73, Loss: 4.535006523132324\n",
      "Epoch: 74, Loss: 4.491861820220947\n",
      "Epoch: 75, Loss: 4.447157859802246\n",
      "Epoch: 76, Loss: 4.3998188972473145\n",
      "Epoch: 77, Loss: 4.3516130447387695\n",
      "Epoch: 78, Loss: 4.304440975189209\n",
      "Epoch: 79, Loss: 4.263869762420654\n",
      "Epoch: 80, Loss: 4.218368053436279\n",
      "Epoch: 81, Loss: 4.17296028137207\n",
      "Epoch: 82, Loss: 4.131771087646484\n",
      "Epoch: 83, Loss: 4.0845208168029785\n",
      "Epoch: 84, Loss: 4.03904390335083\n",
      "Epoch: 85, Loss: 4.00363302230835\n",
      "Epoch: 86, Loss: 3.952315092086792\n",
      "Epoch: 87, Loss: 3.9126133918762207\n",
      "Epoch: 88, Loss: 3.8639538288116455\n",
      "Epoch: 89, Loss: 3.8208460807800293\n",
      "Epoch: 90, Loss: 3.7771739959716797\n",
      "Epoch: 91, Loss: 3.7377805709838867\n",
      "Epoch: 92, Loss: 3.6980769634246826\n",
      "Epoch: 93, Loss: 3.6489474773406982\n",
      "Epoch: 94, Loss: 3.6136057376861572\n",
      "Epoch: 95, Loss: 3.573009490966797\n",
      "Epoch: 96, Loss: 3.531675100326538\n",
      "Epoch: 97, Loss: 3.483210802078247\n",
      "Epoch: 98, Loss: 3.453529119491577\n",
      "Epoch: 99, Loss: 3.4058024883270264\n",
      "Epoch: 100, Loss: 3.36550235748291\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, output_lib_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
