{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2150188401(2) Artificial Intelligence Assignment #2-2<br> Training Vision Transformers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Computer Science & Engineering, Soongsil University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Haneul Pyeon, September 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **PyTorch**. <br>\n",
    "In this notebook, you will learn to understand and build the basic components of Vision Tranformer(ViT). Then, you will try to classify images in the FashionMNIST datatset and explore the effects of different components of ViTs.\n",
    "<br>\n",
    "There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results. \n",
    "\n",
    "### Some helpful tutorials and references for assignment #2-2:\n",
    "- [1] Pytorch official documentation. [[link]](https://pytorch.org/docs/stable/index.html)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Alexey Dosovitskiy et al., \"An Image is Worth 16 x 16 Words: Transformers for Image Recognition at Scale\", ICLR 2021. [[pdf]](https://arxiv.org/pdf/2010.11929.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Vision Transformer\n",
    "Here, you will build the basic components of Vision Transformer(ViT). <br>\n",
    "\n",
    "![Vision Transformer](imgs/ViT.png)\n",
    "\n",
    "Using the explanation and code provided as guidance, <br>\n",
    "Define each component of ViT. <br>\n",
    "\n",
    "\n",
    "#### ViT architecture:\n",
    "* ViT model consists with input patch embedding, positional embeddings, transformer encoder, etc.\n",
    "* Patch embedding\n",
    "* Positional embeddings\n",
    "* Transformer encoder with\n",
    "    * Attention module\n",
    "    * MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Patch Embed\n",
    "\n",
    "**Initialization**: When you create an instance of the PatchEmbedding class, you specify the image_size, patch_size, and in_channels. image_size is the height and width of the input image, patch_size is the size of each patch, and in_channels is the number of input image channels (e.g., 3 for RGB images). \n",
    "\n",
    "**Convolutional Projection**: Inside the PatchEmbedding class, a 2D convolutional layer (nn.Conv2d) is used to perform a patch-based projection. This convolutional layer has a kernel size of patch_size, which defines the size of each patch, and a stride of patch_size, which ensures that patches do not overlap. The convolutional layer effectively extracts image patches.\n",
    "\n",
    "**Reshaping**: After the convolutional projection, the output tensor is reshaped using view. It is transformed from a 4D tensor with dimensions (batch_size, in_channels, H, W) to a 3D tensor with dimensions (batch_size, num_patches, patch_dim). num_patches is the total number of non-overlapping patches in the image, and patch_dim is the number of output channels from the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention\n",
    "\n",
    "**Initialization**\n",
    "* dim: The input dimension of the sequence. This is the dimensionality of the queries, keys, and values.\n",
    "* num_heads: The number of attention heads to use. Multi-head attention allows the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "**Linear Projections (qkv and proj)**: The qkv linear layer takes the input sequence and projects it into three parts: queries (q), keys (k), and values (v). The output of this layer has a shape of (batch_size, sequence_length, 3 * dim).\n",
    "\n",
    "**Forward Pass (forward method)**: In the forward pass, the input tensor x is processed through the attention mechanism. Here's what happens:<br>\n",
    "* The linear projection qkv is applied to x, producing a tensor of shape (batch_size, sequence_length, 3 * dim).|\n",
    "* This tensor is reshaped to have dimensions (batch_size, sequence_length, 3, num_heads, head_dim). The permute operation rearranges the dimensions to (3, batch_size, num_heads, sequence_length, head_dim), making it suitable for multi-head attention.\n",
    "* The three parts, q, k, and v, are extracted from the reshaped tensor.\n",
    "* The attention scores are computed by taking the dot product of queries q and keys k. The result is scaled by self.scale.\n",
    "* The attention scores are passed through a softmax activation along the last dimension (sequence_length), producing attention weights.\n",
    "* The weighted sum of values v is computed using the attention weights.\n",
    "* The result is transposed and reshaped to its original shape, and then passed through the proj linear layer.\n",
    "* The final output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim = -1)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C) # Vector Matrix Multiply\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP\n",
    "\n",
    "The MLP module must consist of three layers:\n",
    "* fully conncted layer 1\n",
    "* activation layer\n",
    "* fully conncted layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer() # GELU\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, out_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Block\n",
    "The transformer block contains the attention module and MLP module which have residual connections. \n",
    "Refer to the following image and build the forward pass.\n",
    "\n",
    "![Transformer Block](imgs/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        # 1. Residual connection for \"Attention\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        # 2. Residual connection for MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vision Transformer\n",
    "\n",
    "Using all the components that you built above, **complete** the vision transformer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.cat\n",
    "\n",
    "Concatenates the given sequence of tensors along the specified dimension. All tensors must either have the same shape (except in the concatenating dimension) or be a 1-D empty tensor with size (0,).\n",
    "\n",
    "`torch.cat()` can be seen as an inverse operation for `torch.split()` and `torch.chunk()`.\n",
    "\n",
    "`torch.cat()` can be best understood via examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "- **tensors** (sequence of Tensors): any Python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the concatenating dimension.\n",
    "\n",
    "- **dim** (int, optional): the dimension over which the tensors are concatenated.\n",
    "\n",
    "#### Keyword Arguments\n",
    "\n",
    "- **out** (Tensor, optional): the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5463,  0.3333,  0.3193],\n",
       "        [ 0.7136, -1.1470, -0.6931]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5463,  0.3333,  0.3193],\n",
       "        [ 0.7136, -1.1470, -0.6931],\n",
       "        [ 0.5463,  0.3333,  0.3193],\n",
       "        [ 0.7136, -1.1470, -0.6931],\n",
       "        [ 0.5463,  0.3333,  0.3193],\n",
       "        [ 0.7136, -1.1470, -0.6931]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam1 = torch.cat((x, x, x), 0)\n",
    "exam1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5463,  0.3333,  0.3193,  0.5463,  0.3333,  0.3193,  0.5463,  0.3333,\n",
       "          0.3193],\n",
       "        [ 0.7136, -1.1470, -0.6931,  0.7136, -1.1470, -0.6931,  0.7136, -1.1470,\n",
       "         -0.6931]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam2 = torch.cat((x, x, x), 1)\n",
    "exam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=10, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm, ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ############################################################################## \n",
    "        # similarly to cls_token, define a learnable positional embedding that matches the patchified input token size.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,  norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Concatenate class tokens to patch embedding\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "\n",
    "        # Add positional embedding to patches\n",
    "        x = x + self.pos_embed \n",
    "\n",
    "        # Forward through encoder blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # Use class token for classification\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Shape: (B, embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        x = self.head(cls_token_final)  # Shape: (B, num_classes)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a small ViT model on FashionMNIST dataset.\n",
    "\n",
    "Define and Train a vision transformer on FashionMNIST dataset. **(You must reach above 85% for full points.)** <br>\n",
    "Train with at least 5 different hyperparameter settings varying the following ViT hyperparameters. \n",
    "Report the setting for the best performance.\n",
    "\n",
    "#### ViT hyperparameters:\n",
    "* patch_size\n",
    "* embed_dim\n",
    "* depth\n",
    "* num_heads\n",
    "* mlp_ratio\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 1/25 [03:01<1:12:27, 181.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 loss: 0.75\n",
      "New best model at epoch 1 with loss 0.7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 2/25 [06:06<1:10:26, 183.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 loss: 0.47\n",
      "New best model at epoch 2 with loss 0.4715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 3/25 [09:15<1:08:14, 186.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 loss: 0.42\n",
      "New best model at epoch 3 with loss 0.4187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 4/25 [12:26<1:05:50, 188.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 loss: 0.39\n",
      "New best model at epoch 4 with loss 0.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 5/25 [15:36<1:02:52, 188.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 loss: 0.37\n",
      "New best model at epoch 5 with loss 0.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 6/25 [18:52<1:00:35, 191.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 loss: 0.36\n",
      "New best model at epoch 6 with loss 0.3557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 7/25 [22:09<57:54, 193.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 loss: 0.34\n",
      "New best model at epoch 7 with loss 0.3435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 8/25 [25:47<56:57, 201.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 loss: 0.34\n",
      "New best model at epoch 8 with loss 0.3390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 9/25 [29:38<56:04, 210.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 loss: 0.34\n",
      "New best model at epoch 9 with loss 0.3359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 10/25 [33:48<55:40, 222.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 loss: 0.33\n",
      "New best model at epoch 10 with loss 0.3285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 11/25 [37:24<51:25, 220.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 loss: 0.32\n",
      "New best model at epoch 11 with loss 0.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 12/25 [41:12<48:18, 222.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 loss: 0.32\n",
      "New best model at epoch 12 with loss 0.3187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 13/25 [44:57<44:40, 223.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 loss: 0.31\n",
      "New best model at epoch 13 with loss 0.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 14/25 [48:40<40:56, 223.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 loss: 0.31\n",
      "New best model at epoch 14 with loss 0.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 15/25 [52:16<36:52, 221.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 loss: 0.31\n",
      "New best model at epoch 15 with loss 0.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 16/25 [55:52<32:55, 219.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 loss: 0.29\n",
      "New best model at epoch 16 with loss 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 17/25 [59:26<29:02, 217.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 loss: 0.29\n",
      "New best model at epoch 17 with loss 0.2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 18/25 [1:02:53<25:03, 214.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 loss: 0.29\n",
      "No improvement in loss for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 19/25 [1:06:14<21:03, 210.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 loss: 0.28\n",
      "New best model at epoch 19 with loss 0.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 20/25 [1:09:40<17:25, 209.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 loss: 0.28\n",
      "New best model at epoch 20 with loss 0.2764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 21/25 [1:13:20<14:10, 212.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 loss: 0.28\n",
      "No improvement in loss for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 22/25 [1:17:18<11:00, 220.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 loss: 0.27\n",
      "New best model at epoch 22 with loss 0.2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 23/25 [1:21:11<07:27, 223.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 loss: 0.27\n",
      "New best model at epoch 23 with loss 0.2678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 24/25 [1:24:50<03:42, 222.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 loss: 0.26\n",
      "New best model at epoch 24 with loss 0.2622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 25/25 [1:28:29<00:00, 212.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 loss: 0.26\n",
      "New best model at epoch 25 with loss 0.2558\n",
      "Best model saved with loss 0.2558\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:13<00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.36\n",
      "Test accuracy: 86.71%\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Train():\n",
    "    ##############################################################################\n",
    "    #                           IMPLEMENT YOUR CODE                              #\n",
    "    ##############################################################################\n",
    "\n",
    "    patch_size = 8\n",
    "    embed_dim = 360\n",
    "    depth = 8\n",
    "    num_heads = 12 # make sure embed_dim is divisible by num_heads!\n",
    "    mlp_ratio = 4.0\n",
    "\n",
    "    ##############################################################################\n",
    "    #                              END YOUR CODE                                 #\n",
    "    ##############################################################################\n",
    "\n",
    "    # Loading data\n",
    "    transform = ToTensor()\n",
    "\n",
    "    train_set = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "    # Defining model and training options\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "    model = VisionTransformer(patch_size=patch_size, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio).to(device)\n",
    "    model_path = './vit.pth'\n",
    "    N_EPOCHS = 25\n",
    "    LR = 0.0005\n",
    "    early_stopping_patience = 3  # Early stopping patience (number of epochs with no improvement)\n",
    "    min_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    train_loss_history = []  # To store train losses for graph\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = AdamW(model.parameters(), lr = LR, weight_decay = 5e-3)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if train_loss < min_loss:\n",
    "            min_loss = train_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f'New best model at epoch {epoch + 1} with loss {min_loss:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement in loss for {patience_counter} epochs.')\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save the best model after training\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, model_path)\n",
    "        print(f'Best model saved with loss {min_loss:.4f}')\n",
    "\n",
    "    # Plotting the training loss\n",
    "    plt.plot(range(1, len(train_loss_history) + 1), train_loss_history, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test loop\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the best model before testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did and discovered here\n",
    "In this cell you should write all the settings tried and performances you obtained. Report what you did and what you discovered from the trials.\n",
    "You can write in Korean and English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1\n",
    "### Hyperparameters Setting\n",
    "- patch_size : 8\n",
    "- embed_dim : 360\n",
    "- depth : 8\n",
    "- num_heads : 12\n",
    "- mlp_ratio : 4.0\n",
    "\n",
    "### Results\n",
    "- Training time : about 88 min.\n",
    "- Test loss : 0.36\n",
    "- Test accuracy : 86.71%\n",
    "\n",
    "\n",
    "### P.S.\n",
    "Due to the unavailability of GPU (CPU usage) in my local environment, please understand that I am attaching a model that was trained only once with the hyperparameter settings shown in the source code above in the local environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
