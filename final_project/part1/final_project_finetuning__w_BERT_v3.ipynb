{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lynO3lHBsIUb"
      },
      "source": [
        "# 2150188401(2) Artificial intelligence Final Project: Finetuning_BERT\n",
        "\n",
        "Copyright (C) Computer Science & Engineering, Soongsil University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Haneul Pyeon, November 2024.\n",
        "\n",
        "BERT(Bidirectional Encoder Representations from Transformers) is a groundbreaking model in the NLP domain. This tutorial provides a step-by-step guide on how to fine-tune the lightweight BERT variant using Hugging Face's transformers library for text classification tasks.<br>\n",
        "\n",
        "This is about BERT (Devlin et al., 2018).<br>\n",
        "https://arxiv.org/abs/1810.04805\n",
        "\n",
        "The code below are based on the following link. <br>\n",
        "https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n",
        "\n",
        "\n",
        "### Fine-tune the model\n",
        "1. Design your model's prediction head\n",
        "2. Finetune the model by changing the hyperparameters.\n",
        "3. You will get a score based on the your (hidden) test accuracy for text classification (ranking-based).  \n",
        "\n",
        "### Submitting your work:\n",
        "<font color=red>**DO NOT clear the final outputs**</font> so that we can grade both your code and results.  \n",
        "\n",
        "\n",
        "Now proceed to the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "wdD_wsaRsIUd"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GM2VcbhdsIUe"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "345EalxjsIUf",
        "outputId": "c24f999a-6474-44e4-b5c3-833564d2640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install pandas\n",
        "!python3 -m pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00CLz-szsIUf"
      },
      "source": [
        "### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oBPHajtqsIUf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rVinMhosIUg"
      },
      "source": [
        "### Specify your GPU number if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3x3LONCsIUh",
        "outputId": "dc2ec948-27cd-4e34-d08d-39c2b674692e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES = 0\n",
        "\n",
        "if torch.cuda.is_available() is True:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HjEYsyttsIUh"
      },
      "source": [
        "## Preparing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXt1VHQosIUi"
      },
      "source": [
        "link : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "1. Download the dataset from attached link.\n",
        "2. Move the downloaded zip file under the \"data\" directory and then unzip the zip file.\n",
        "3. Run the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AHCwVo4gsIUi"
      },
      "outputs": [],
      "source": [
        "def load_imdb_data(data_file_path):\n",
        "    if os.path.exists(data_file_path):\n",
        "        df = pd.read_csv(data_file_path)\n",
        "        texts = df['review'].tolist()\n",
        "        labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
        "        return texts, labels\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"The file '{data_file_path}' does not exist.\")\n",
        "\n",
        "data_file_path = './data/IMDB Dataset Train.csv'\n",
        "texts, labels = load_imdb_data(data_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0zR69vssIUj"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SGtEEz4_sIUj"
      },
      "outputs": [],
      "source": [
        "class CustomTextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_seq_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_seq_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "_FzTG6NysIUj"
      },
      "source": [
        "## Classifier head for BERT( Design your model's prediction head )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EsfrNagasIUj"
      },
      "outputs": [],
      "source": [
        "class CustomBERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(CustomBERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        ######################## TO-DO ########################\n",
        "\n",
        "        # 1. Hidden Layer\n",
        "        self.hidden_layer = nn.Linear(self.bert.config.hidden_size, 768)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # 2. Dropout Layer\n",
        "        self.dropout = nn.Dropout(p = 0.15)\n",
        "\n",
        "        # 3. Output Layer\n",
        "        self.output_layer = nn.Linear(768, num_classes)\n",
        "\n",
        "        ######################## TO-DO ########################\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        ######################## TO-DO ########################\n",
        "\n",
        "        # 1. Hidden Layer + Activation\n",
        "        x = self.hidden_layer(pooled_output)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # 2. Dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 3. Output Layer\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        ######################## TO-DO ########################\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ZA1UPOq3sIUj"
      },
      "source": [
        "## train and evaluation method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NBJxvXAhsIUj"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for batch in tqdm(data_loader, desc=\"Train\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        ######################## TO-DO ########################\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        ######################## TO-DO ########################\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diJiY2wssIUj"
      },
      "source": [
        "## Hyper-parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YROABySCsIUj"
      },
      "outputs": [],
      "source": [
        "# Set up parameters\n",
        "# Hint: generally, 5 ~ 10 epochs will be enough.\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "######################## TO-DO ########################\n",
        "max_seq_length = 500\n",
        "batch_size = 8\n",
        "num_epochs = 2\n",
        "learning_rate = 2e-5\n",
        "######################## TO-DO ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CkllW2AosIUk"
      },
      "source": [
        "## get data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G3y0aZ0WsIUk"
      },
      "outputs": [],
      "source": [
        "######################## DO NOT CHANGE ########################\n",
        "train_texts, val_texts, train_labels, val_labels = \\\n",
        "train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "######################## DO NOT CHANGE ########################\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = CustomTextClassificationDataset(train_texts, train_labels, tokenizer, max_seq_length)\n",
        "val_dataset = CustomTextClassificationDataset(val_texts, val_labels, tokenizer, max_seq_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWHZ50_6sIUk"
      },
      "source": [
        "## Define model, optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYTgLHxfsIUk",
        "outputId": "814fafc4-bd32-46c9-859f-ad8baeb2dead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = CustomBERTClassifier(bert_model_name, num_classes).to(device)\n",
        "######################## TO-DO ########################\n",
        "optimizer = AdamW(model.parameters(), lr = learning_rate, weight_decay = 0.01)\n",
        "######################## TO-DO ########################\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "e8UAOgPtsIUk"
      },
      "source": [
        "## Train model and save best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mDhaJNcusIUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a65511-c01b-4586-ba3b-bb3ff84629d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 4000/4000 [52:29<00:00,  1.27it/s]\n",
            "Validation: 100%|██████████| 1000/1000 [04:44<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9321\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      3911\n",
            "           1       0.96      0.90      0.93      4089\n",
            "\n",
            "    accuracy                           0.93      8000\n",
            "   macro avg       0.93      0.93      0.93      8000\n",
            "weighted avg       0.93      0.93      0.93      8000\n",
            "\n",
            "Saved Trained Model.\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 4000/4000 [52:22<00:00,  1.27it/s]\n",
            "Validation: 100%|██████████| 1000/1000 [04:41<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9440\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94      3911\n",
            "           1       0.94      0.95      0.95      4089\n",
            "\n",
            "    accuracy                           0.94      8000\n",
            "   macro avg       0.94      0.94      0.94      8000\n",
            "weighted avg       0.94      0.94      0.94      8000\n",
            "\n",
            "Saved Trained Model.\n"
          ]
        }
      ],
      "source": [
        "eval_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model_path = './finetuned_bert.pth'\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_model(model, train_dataloader, optimizer, scheduler, device)\n",
        "    accuracy, report = evaluate_model(model, val_dataloader, device)\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(report)\n",
        "\n",
        "    if eval_acc < accuracy:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print('Saved Trained Model.')\n",
        "        eval_acc = accuracy"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}